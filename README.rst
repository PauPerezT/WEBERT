
========
WEBERT
========

.. image:: https://github.com/PauPerezT/WEBERT/blob/master/logos/logo_web.png?raw=true

Getting BERT embeddings from Transformers.

"WEBERT: Word Embedding using BERT"
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

WEBERT is a python toolkit designed to help students in order to compute dynamic and static Bidirectional Encoder Representations from Transformers (BERT) embeddings using Transformers (https://github.com/huggingface/transformers). WEBERT is avalable for english and spanish (multilingual) models, as well as for base and large models, and  cased and lower-cased cases. BETO and SciBERT are also avalaible here. BETO is a pretrained BERT model from spanish corpus (https://github.com/dccuchile/beto).SCIBERT is a pre-trained model on english scientific text (https://github.com/allenai/scibert). The static features are computed per each neuron based on the mean, standar deviation, kurtosis, skewness, min and max. The project is ongoing.

The code for this project is available at https://github.com/PauPerezT/WEBERT

From this repository::

    git clone https://github.com/PauPerezT/WEBERT
    
Install
^^^^^^^

To install the requeriments, please run::

    install.sh
